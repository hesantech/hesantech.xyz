[{"content":"Kubernetes is a free open source container orchestration tool. It helps to manage connectivity between containers and automatically scale up and scale down containers based on load\nEnvironment  VirtualBox Version 6.1.26 Ubuntu server 20.04 LTS Kuberentes version 1.21  Requirements  2CPU for each node 2GB Ram Disable swap 15GB HDD for each node(As per requirement) Working internet connection Unique hostname, MAC address, and product_uuid for every node. See here for more details.  I\u0026rsquo;m going to deploy this kubernets cluster with 1 master VM and 2 worker VMs, each of the VM will have 2 network adapters one with bridge network and other with virtualbox NAT network.\nBridge Network: Enables us to connect to nodes directly from home router network.\nVirtualbox NAT Network: This is a virtualbox internal NAT network all VMs within this network can communicate with each other and also communicate to internet but any machine from external network as well as physical network where host machine is connected cannot access the VMs(guest os) configured with NAT network (i.e Basically NAT Network is a virtual switch act as a gateway to internal network)\nWe can use bridge network itself but the problem with the bridge network is it always requires the home network connection and making any configuration changes to home network like switching to a different network CIDR, changes the IP of the K8s master will breaks down the entire setup. Since certificates during the K8s setup are tied to a specific IP and need to generated again each time the IP address of the master changes its a tedious task and using NAT network also enable k8s to work without any host network dependency. Hence using NAT Network.\nNetwork Setup Home Network: 192.168.0.1/24\nCreate a virtualbox NAT network 192.168.100.0/24 different than the home network address to avoid conflict with home network as shown below:    Node Home Network Virtualbox Nat Network     k8s-master-0 192.168.1.80 192.168.100.1   k8s-worker-1 192.168.1.90 192.168.100.2   k8s-worker-2 192.168.1.91 192.168.100.3    Steps   Download the ubuntu server 20.04 LTS for all the master and worker nodes.\n  After creating the VM with downloaded OS enable both network adapter in virtualbox as shown below\n  Begin the OS installation, with default options for language and IP with DCHP we will change this to static after setup\n  Create partitions as shown below:   Configure the profile details as shown below:   Configure the network to be static as shown below, Ubuntu server 20.04 by default uses netplan to manage network configurations in the file /etc/netplan/00-installer-config.yaml once edited as above sudo netplan apply to apply config and verifiy with ip link or ifconfig\n  Disable the swap space.\n  sudo swapon -s # check current swap status sudo swapoff -a # disable the swap sudo swapon -s # verify again Also comment the swap entry in /etc/fstab as shown below: Its good to take a snapshot now if anything goes wrong we can use this snapshot to restore current state from few clicks and also we are going to create worker VMs from this snapshot and ensure that you are creating clone with generating new mac address as shown below  Once both worker VMs create update its hostname, ip address and add host entries to /etc/hostsas shown below  We must verify that every VMs got unique MAC address ip link or ifconfig -a and product_uuid can be checked by using the command sudo cat /sys/class/dmi/id/product_uuid Lets setup containerd as CRI runtime for all nodes, execute the below commands on each nodes with root privilege for containerd prerequisites.  cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # Setup required sysctl params, these persist across reboots. cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # Apply sysctl params without reboot sudo sysctl --system Set up the repository for containerd as mentioned below  sudo apt-get update sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  gnupg \\  lsb-release #Add Docker’s official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg # set up the **stable** repository echo \\  \u0026#34;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null Install and configure containerd  #Install containerd sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y containerd #Configure sudo mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml #Restart containerd: sudo systemctl restart containerd Once CRI runtime setup completed we can install kubernetes repo as follows to install kubernetes  #Download the Google Cloud public signing key: sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg #Add the Kubernetes `apt` repository: echo \u0026#34;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; | sudo tee /etc/apt/sources.list.d/kubernetes.list At the time of writing this post latest kubernetes release verison is 1.22 but I\u0026rsquo;m going to install kubernetes version 1.21, So i can practice version update. If you need to install current version install without mentioning version and skip kubectl installation on worker nodes.  #Update `apt` package index, install kubelet, kubeadm and kubectl, and pin their version: sudo apt-get update sudo apt-get install -y kubelet=1.21.0-00 kubeadm=1.21.0-00 kubectl=1.21.0-00 sudo apt-mark hold kubelet kubeadm kubectl #verify the installed versions kubelet --version kubeadm version -o short kubectl version We are going to initialize cluster using kubeadm. (Execute only in master node)  #Intitate the cluster sudo kubeadm init --control-plane-endpoint k8s-master-0:6443 --pod-network-cidr 10.10.0.0/16 #In the above command POD CIDR is taken as 10.10.0.0/16, these IP address is completely internal to kubernetese so can assign any IP CIDR. #Once inilization completed we need to run the following as a regular user to start using kubectl: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Lets setup pod network (internal virtual network spans across nodes). In this setup we are going to install Calico network plugin it supports network policy and major cloud providers supports calico, You can read more about Calico and calico introduced new installation option from version 3.15 we can install calico using open source operator created by tigera i\u0026rsquo;m not used that option in this setup but you can find more details here Operator Setup By default calico uses the CIDR 192.168.0.0/16 as pod network, if you initialized the cluster with the CIDR 192.168.0.0/16 you can directly install calico with kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml. But my home network already resides on 192.168.0.0 so to avoid conflict we initialized cluster with 10.10.0.0./16 that need to be updated in calico also to do that we need to uncomment a variable CALICO_IPV4POOL_CIDR in calico installation manifest file and update the CIDR same as initialization pod network (i.e 10.10.0.0/16) as follows.  #Download the Calico networking manifest curl https://docs.projectcalico.org/manifests/calico.yaml -O #vim calico.yaml and update the variable as mentioned above. #Install  kubectl apply -f calico.yaml Lets add worker nodes to the cluster. you can use initialization step16\u0026rsquo;s output which contains the command to join worker nodes to the cluster or create a new token usingkubeadm token create --print-join-command and execute the kubeadm join \u0026lt;hash value\u0026gt; command on worker nodes as root user. Once worker node joined to cluster check with kubectl get nodes and try to create a test pod kubectl run nginx --image=nginx to verify everything is working fine If anything goes wrong execute this kubeadm reset \u0026amp;\u0026amp; rm -rf /etc/cni/net.d on master and worker nodes and retry all the steps.  ","permalink":"https://hesantech.xyz/posts/2021-09/26_kubernetes_cluster_setup/","summary":"Kubernetes is a free open source container orchestration tool. It helps to manage connectivity between containers and automatically scale up and scale down containers based on load\nEnvironment  VirtualBox Version 6.1.26 Ubuntu server 20.04 LTS Kuberentes version 1.21  Requirements  2CPU for each node 2GB Ram Disable swap 15GB HDD for each node(As per requirement) Working internet connection Unique hostname, MAC address, and product_uuid for every node. See here for more details.","title":"kubernetes Cluster Setup"},{"content":"You can get in touch with or find me at:\n [LinkedIn] [Github]  ","permalink":"https://hesantech.xyz/contact/","summary":"You can get in touch with or find me at:\n [LinkedIn] [Github]  ","title":"Contact"}]